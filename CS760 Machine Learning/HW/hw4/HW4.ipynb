{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import matplotlib as plt;\n",
    "import pandas as pd;\n",
    "\n",
    "class Naive_Bayes:\n",
    "    def __init__(self, theta, prior):\n",
    "        ## theta is the dictionary; so is prior\n",
    "        self.theta = theta;\n",
    "        self.theta_len = len(theta);\n",
    "        \n",
    "        self.prior = prior;\n",
    "        self.prior_len = len(prior);\n",
    "        self.train = {}; ## for counting the freq\n",
    "        for i in prior:\n",
    "            self.train[i] = np.array([0] * self.theta_len); # now it is an array, but finally will convert to a list\n",
    "        self.prior_cts = np.array([0] * self.prior_len);\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # set training\n",
    "    # 1.\n",
    "    def Load_data(self,pathname, filename):\n",
    "        text_len = {};\n",
    "        # this record the total length of the training texts in all the priors\n",
    "        for i in self.prior:\n",
    "            text_len[i] = 0;\n",
    "        \n",
    "        for i in range(len(filename)):\n",
    "            ## count the freq of prior            \n",
    "            label = filename[i][0]; ## it is 'e' 'j' 's' in this case\n",
    "            \n",
    "            full_name = pathname + filename[i] + \".txt\"\n",
    "            filetest = open(full_name,\"r\");\n",
    "            text = filetest.read();\n",
    "            filetest.close();\n",
    "            \n",
    "            cts = self.Counting_theta(text); # this is a list\n",
    "            text_len[label] += len(text);\n",
    "            #text_len[label] += sum(cts);\n",
    "            \n",
    "            self.train[label] = (self.train[label]) + np.array(cts); \n",
    "            ## convert to array first and do the vectorization calculation    \n",
    "            \n",
    "        for label in self.train:\n",
    "            self.train[label] = self.getDistribution(self.train[label],text_len[label],self.theta_len); \n",
    "        \n",
    "        ## save the prior in self.prior_cts\n",
    "        r = 0;\n",
    "        for i in self.train:\n",
    "            self.prior_cts[r] = len(self.train[i]) / len(filename);\n",
    "            r += 1;\n",
    "            \n",
    "        self.prior_cts = self.getDistribution(self.prior_cts, len(filename), (self.prior_len));        \n",
    "      \n",
    "    # 2.    \n",
    "    # return a list counts the number of features\n",
    "    def Counting_theta(self, text):\n",
    "        cts = [0] * (self.theta_len);\n",
    "        for j in text:\n",
    "            if j in self.theta:\n",
    "                cts[self.theta[j]] += 1;  \n",
    "            #if j == '\\n':\n",
    "                #cts[26] += 1;\n",
    "        return cts;\n",
    "    # 3.            \n",
    "    def getDistribution(self,cts, N, V):\n",
    "        #cts = self.Counting_theta(text);\n",
    "        #cts = self.Add_1_Smooth(cts,len(text),len(self.theta));        \n",
    "        cts = self.Add_1_Smooth(cts,N,V);\n",
    "        cts /= np.sum(cts); ## normalize\n",
    "        \n",
    "        cts = np.log10(cts); ## out as log        \n",
    "        \n",
    "        return cts;\n",
    "        \n",
    "    # 4.    \n",
    "    def Add_1_Smooth(self,Cws,N,V):\n",
    "        ## Cws is the count of the word w\n",
    "        ## N is total number of words\n",
    "        ## V is the vocabulary size (distinct words)\n",
    "        \n",
    "        return ((Cws + 1) / (N + V)); \n",
    "    \n",
    "    # test\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # 5\n",
    "    def Load_test(self,pathname,filename):\n",
    "        # filename is a list, thus it can deal with multiple files at the same time\n",
    "        self.test_cts = [0] * (len(filename));\n",
    "        for i in range(len(filename)):\n",
    "            ## count the freq of prior            \n",
    "            #label = filename[i][0];\n",
    "            \n",
    "            full_name = pathname + filename[i] + \".txt\"\n",
    "            filetest = open(full_name,\"r\");\n",
    "            text = filetest.read();\n",
    "            filetest.close();\n",
    "            \n",
    "            self.test_cts[i] = self.Counting_theta(text);  \n",
    "            self.test_cts[i] = np.array(self.test_cts[i]);\n",
    "            # the counts of test case\n",
    "    \n",
    "    # 6        \n",
    "    # this function calculate the \"likelihood and posterior\" of a single test case \n",
    "    def Classify(self,cts):\n",
    "        posterior = [0] * self.prior_len;\n",
    "        likelihood = [0] * self.prior_len;\n",
    "        \n",
    "        def getProb(cts,theta):                        \n",
    "            return cts.dot(theta);\n",
    "        \n",
    "        r = 0; ## r is the r_th language, \n",
    "        for i in self.train: ## i is exactly the language, 's' 'j' 'e' in this case\n",
    "            likelihood[r] = getProb(cts, self.train[i]);\n",
    "            posterior[r] = likelihood[r] + (self.prior_cts[r]);\n",
    "            r += 1;        \n",
    "            \n",
    "        return [likelihood, posterior];   # pred[i] means the log likelihood of the i_th language\n",
    "    \n",
    "    # 7 \n",
    "    # using 6\n",
    "    # filename is a list, thus it will deal with a series of files and return their likelihood and posterior as a list\n",
    "    def Pred(self,pathname,filename):        \n",
    "        result = [];\n",
    "        for i in self.prior:\n",
    "            result.append(i);\n",
    "        self.Load_test(pathname,filename);       \n",
    "        \n",
    "        self.likelihood = [0] * len(filename); ## this is p(x|y)\n",
    "        self.posterior = [0] * len(filename);  ## this is p(x|y) * p(y) = p(y|x);\n",
    "        self.pred_list = [0] * len(filename);  ## this is the index of y\n",
    "        self.pred_label = [0] * len(filename); ## this is the name of this y\n",
    "        for i in range(len(filename)):\n",
    "            [like, post] = self.Classify(self.test_cts[i]); \n",
    "            self.likelihood[i] = like;\n",
    "            self.posterior[i] = post;\n",
    "            self.pred_list[i] = post.index(max(post));\n",
    "            self.pred_label[i] = result[self.pred_list[i]];\n",
    "    #---------------------------------------------------------------------------------------        \n",
    "    # 8        \n",
    "    def Bag_of_words(self,counts): ## counting the frequency of each char for a testing case\n",
    "        vector = {};\n",
    "        r = 0;        \n",
    "        for i in self.theta:\n",
    "            vector[i] = counts[r];\n",
    "            r += 1;\n",
    "        print(vector);\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # 9 \n",
    "    ## this is used to get the confusion matrix\n",
    "    ## only when you know the true labels of the test cases!!!\n",
    "    def Confusion_matrix(self, filename):\n",
    "        y_true =[];\n",
    "        for i in filename:\n",
    "            label = i[0];\n",
    "            y_true.append(self.prior[label]);\n",
    "            \n",
    "        rank = self.prior_len;\n",
    "        mat = np.zeros((rank,rank));\n",
    "        for i in range(len(y_true)):\n",
    "            mat[self.pred_list[i],y_true[i]] += 1;\n",
    "        \n",
    "        mat = mat.astype(int);        \n",
    "        labels = [];\n",
    "        for i in self.prior: ## making dataframe\n",
    "            labels.append(i);\n",
    "        \n",
    "        frame = {};\n",
    "        for i in range(self.prior_len):\n",
    "            series = pd.Series(mat[:,i], index = labels);\n",
    "            frame[labels[i]] = series;\n",
    "        \n",
    "        df = pd.DataFrame(frame);        \n",
    "        return df;\n",
    "            \n",
    "            \n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # 10    \n",
    "    def Text_shuffle(self,text):\n",
    "        text =list(text);\n",
    "        return ''.join(np.random.shuffle(text));\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # 11\n",
    "    ## prepare WEKA text files\n",
    "    def Weka_prep(self,pathname_read,filename,pathname_save):\n",
    "        def writeFile(fname, content):\n",
    "            f = open(fname, 'w');\n",
    "            f.write(content);\n",
    "            f.close()\n",
    "        \n",
    "        def Space(text):\n",
    "            text = list(text);            \n",
    "            for i in range(len(text)):                \n",
    "                if text[i] == ' ':\n",
    "                    text[i] = 'space';\n",
    "            return ' '.join(text);\n",
    "        \n",
    "        for i in range(len(filename)):        \n",
    "            label = filename[i][0];\n",
    "            \n",
    "            full_name = pathname_read + filename[i] + \".txt\"\n",
    "            filetest = open(full_name,\"r\");\n",
    "            text = filetest.read();\n",
    "            filetest.close();\n",
    "            \n",
    "            text = Space(text);\n",
    "            \n",
    "            full_name = pathname_save + label + '/' + filename[i][1:] + \".txt\";\n",
    "            writeFile(full_name, text);\n",
    "            \n",
    "        print(\"Weka prepared!\");\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = {\"a\":0, \"b\":0,\"c\":0, \"d\":0,\"e\":0,\"f\":0,\"g\":0, \"h\":0,\"i\":0, \"j\":0, \"k\":0, \"l\":0, \"m\":0, \"n\":0, \"o\":0, \"p\":0, \"q\":0, \"r\":0,\n",
    "            \"r\":0, \"s\":0, \"t\":0, \"u\":0, \"v\":0, \"w\":0, \"x\":0, \"y\":0, \"z\":0, \" \": 0};\n",
    "r = 0;\n",
    "for i in alphabet:\n",
    "    alphabet[i] += r;\n",
    "    r += 1;\n",
    "    \n",
    "language = {\"e\":0, \"j\":0, \"s\":0};\n",
    "r = 0;\n",
    "for i in language:\n",
    "    language[i] += r;\n",
    "    r += 1;\n",
    "    \n",
    "training_files = [];\n",
    "for j in language:\n",
    "    for i in range(0,10):\n",
    "        training_files += [j + str(i)];\n",
    "\n",
    "pathname = 'languageID/languageID/languageID/'; \n",
    "## filename = pathname + such as e2 + .txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = Naive_Bayes(alphabet,language);\n",
    "NB.Load_data(pathname,training_files);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "## Q 3.1:\n",
    "print(np.power(10,NB.prior_cts));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_e:\n",
      "{'a': 0.060148, 'b': 0.011158, 'c': 0.021524, 'd': 0.021986, 'e': 0.105308, 'f': 0.018949, 'g': 0.017496, 'h': 0.047207, 'i': 0.055394, 'j': 0.001453, 'k': 0.003763, 'l': 0.028985, 'm': 0.020533, 'n': 0.057903, 'o': 0.064439, 'p': 0.01677, 'q': 0.000594, 'r': 0.05381, 's': 0.066156, 't': 0.080087, 'u': 0.026674, 'v': 0.009309, 'w': 0.015516, 'x': 0.001188, 'y': 0.013865, 'z': 0.00066, 'space': 0.179123}\n",
      "Theta_j:\n",
      "{'a': 0.131676, 'b': 0.010892, 'c': 0.005516, 'd': 0.017245, 'e': 0.060183, 'f': 0.00391, 'g': 0.014033, 'h': 0.031767, 'i': 0.096977, 'j': 0.002374, 'k': 0.05739, 'l': 0.001466, 'm': 0.039796, 'n': 0.056692, 'o': 0.091112, 'p': 0.000908, 'q': 0.00014, 'r': 0.042798, 's': 0.04217, 't': 0.056971, 'u': 0.070586, 'v': 0.000279, 'w': 0.019758, 'x': 7e-05, 'y': 0.014173, 'z': 0.00775, 'space': 0.123368}\n",
      "Theta_s:\n",
      "{'a': 0.104504, 'b': 0.008257, 'c': 0.037525, 'd': 0.039744, 'e': 0.113747, 'f': 0.008627, 'g': 0.007209, 'h': 0.00456, 'i': 0.049849, 'j': 0.006655, 'k': 0.000308, 'l': 0.05293, 'm': 0.025818, 'n': 0.054162, 'o': 0.072463, 'p': 0.024278, 'q': 0.007702, 'r': 0.059277, 's': 0.065747, 't': 0.035615, 'u': 0.033705, 'v': 0.005915, 'w': 0.000123, 'x': 0.002526, 'y': 0.007887, 'z': 0.002711, 'space': 0.168156}\n"
     ]
    }
   ],
   "source": [
    "## Q 3.(2-3)\n",
    "## print the thetas\n",
    "for i in NB.train:\n",
    "    print(\"Theta_%s:\"%(i))\n",
    "    theta = np.power(10,NB.train[i]);   \n",
    "    theta = np.round(theta,6)\n",
    "    ans = {};\n",
    "    r = 0;\n",
    "    for i in NB.theta:\n",
    "        if i == \" \":\n",
    "            ans['space'] = theta[r];\n",
    "        else:\n",
    "            ans[i] = theta[r];\n",
    "        r += 1;\n",
    "    print(ans);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e10', 'e11', 'e12', 'e13', 'e14', 'e15', 'e16', 'e17', 'e18', 'e19', 'j10', 'j11', 'j12', 'j13', 'j14', 'j15', 'j16', 'j17', 'j18', 'j19', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19']\n"
     ]
    }
   ],
   "source": [
    "test_files = [];\n",
    "for j in language:\n",
    "    for i in range(10,20):\n",
    "        test_files += [j + str(i)];\n",
    "print(test_files)\n",
    "NB.Pred(pathname,test_files)\n",
    "#print(NB.pred_list)\n",
    "#NB.pred_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 164, 'b': 32, 'c': 53, 'd': 57, 'e': 311, 'f': 55, 'g': 51, 'h': 140, 'i': 140, 'j': 3, 'k': 6, 'l': 85, 'm': 64, 'n': 139, 'o': 182, 'p': 53, 'q': 3, 'r': 141, 's': 186, 't': 225, 'u': 65, 'v': 31, 'w': 47, 'x': 4, 'y': 38, 'z': 2, ' ': 498}\n"
     ]
    }
   ],
   "source": [
    "## Q 3.4\n",
    "NB.Bag_of_words(NB.test_cts[0]) ## the e10.text is the first element in this list!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3405.6445560384063, -3804.210716450759, -3670.8233803708918]\n",
      "2.267 e -3406\n",
      "6.156 e -3805\n",
      "1.502 e -3671\n"
     ]
    }
   ],
   "source": [
    "## Q 3.5\n",
    "ans = NB.likelihood[0];\n",
    "print(ans) ## the e10.txt is the first element in this list\n",
    "def sci_exp(x):\n",
    "    intx = int(x);\n",
    "    if intx > x:\n",
    "        intx -= 1;\n",
    "    delta = x - intx;\n",
    "    coe = 10 ** delta;\n",
    "    coe = round(coe,3);\n",
    "    print(coe,'e',intx);\n",
    "    \n",
    "for i in ans:\n",
    "    sci_exp(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post :\n",
      " [-3406.121677293126, -3804.6878377054786, -3671.3005016256116]\n",
      "7.557 e -3407\n",
      "2.052 e -3805\n",
      "5.006 e -3672\n",
      "pred_label:\n",
      " e\n"
     ]
    }
   ],
   "source": [
    "## Q 3.6\n",
    "ans = NB.posterior[0];\n",
    "print('post :\\n',ans);\n",
    "for i in ans:\n",
    "    sci_exp(i)\n",
    "print('pred_label:\\n', NB.pred_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nweka_files = [];\\nfor j in language:\\n    for i in range(0,20):\\n        weka_files += [j + str(i)];\\nsave_path =  'Weka/'       \\nNB.Weka_prep(pathname, weka_files, save_path);\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prepare weka files\n",
    "'''\n",
    "weka_files = [];\n",
    "for j in language:\n",
    "    for i in range(0,20):\n",
    "        weka_files += [j + str(i)];\n",
    "save_path =  'Weka/'       \n",
    "NB.Weka_prep(pathname, weka_files, save_path);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>j</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>e</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>j</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>s</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    e   j   s\n",
       "e  10   0   0\n",
       "j   0  10   0\n",
       "s   0   0  10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.Confusion_matrix(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

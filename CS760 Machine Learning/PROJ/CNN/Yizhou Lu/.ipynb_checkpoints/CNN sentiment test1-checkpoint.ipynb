{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTyT_XmmBvKG"
   },
   "outputs": [],
   "source": [
    "import sys;\n",
    "import torch;\n",
    "import torch.nn as nn;\n",
    "import torchtext.data as ttd;\n",
    "from torchtext.vocab import GloVe;\n",
    "\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "pd.options.mode.chained_assignment = None; ## avoid warning\n",
    "#from datetime import datetime;\n",
    "import time;\n",
    "## time.process_time() to record the time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1588369053503,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "qjgtLbZHDRPi",
    "outputId": "4111acf8-8da4-446a-c0d9-434ab4db7fa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os;\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\');\\npath = \"/content/drive/My Drive\";\\nos.chdir(path);\\nos.listdir(path);\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os;\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive');\n",
    "path = \"/content/drive/My Drive\";\n",
    "os.chdir(path);\n",
    "os.listdir(path);\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ud_XSDW-BvKK"
   },
   "source": [
    "## step 1 data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3365,
     "status": "ok",
     "timestamp": 1588369056581,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "iVb8LEZRBvKL",
    "outputId": "e6581c69-5665-4494-8111-a3bec613de25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yizhou said 欲速则不达，施主稍安勿躁: | ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ | 100.0% (^_^)/ Done!"
     ]
    }
   ],
   "source": [
    "def Process_bar(ratio, comments = False, overwrite = True, length = 50):\n",
    "    bar = 'Yizhou said 欲速则不达，施主稍安勿躁: | ';\n",
    "    i = 0;\n",
    "    while i < ratio * length:\n",
    "        bar += '▒';\n",
    "        i += 1;\n",
    "    while i < length:\n",
    "        bar += '░';\n",
    "        i += 1;\n",
    "    \n",
    "    bar += (' | %s%%'%(int(ratio*1000)/10));\n",
    "    if ratio == 1:\n",
    "        bar += ' (^_^)/ Done!'\n",
    "    if comments != False:\n",
    "        bar += ('\\n' + str(comments));\n",
    "    if overwrite == True:\n",
    "        print('\\r', end='');\n",
    "    else:\n",
    "        print('\\n',end = '');\n",
    "    print(bar, end='');\n",
    "    sys.stdout.flush();\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def Twitter_text_clean(text):\n",
    "    \n",
    "    #print(text)\n",
    "    i = 1;\n",
    "    #while text[i-1:i+1] != ': ':\n",
    "    while i < len(text):\n",
    "        if text[i-1:i+1] == ': ':\n",
    "            break;\n",
    "        i += 1;\n",
    "        \n",
    "    if i >= len(text):\n",
    "        return text;\n",
    "    else:   \n",
    "        return text[i+1:];\n",
    "\n",
    "#!wget https://drive.google.com/open?id=1l59-HAyiqb6jcrE7ks-UWAaDn97L7p3l\n",
    "\n",
    "df = pd.read_csv('first-gop-debate-twitter-sentiment/Sentiment.csv');\n",
    "#df = pd.read_csv('/content/drive/My Drive/Code Colab/Sentiment analysis/CNN/first-gop-debate-twitter-sentiment/Sentiment.csv');\n",
    "\n",
    "df['binary_labels'] = df['sentiment'].map({'Neutral': 0, 'Negative':2, 'Positive':1});\n",
    "df2 = df[['binary_labels','text']];\n",
    "\n",
    "print_out = int(len(df2['text'])/100);\n",
    "#print_out = 1;\n",
    "for i in range(len(df2['text'])):\n",
    "    \n",
    "    if i%print_out == 0:\n",
    "        ratio = (i+1)/len(df2['text']);\n",
    "        Process_bar(ratio);\n",
    "    \n",
    "    df2['text'][i] = Twitter_text_clean(df['text'][i]);\n",
    "Process_bar(1);\n",
    "df2.to_csv('twitter.csv', index = False);\n",
    "\n",
    "## classes is 2 now!\n",
    "K_class = 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viNQzF5XFcxS"
   },
   "source": [
    "## Step 2 To Cuda & +ttd\n",
    "Preparaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3829,
     "status": "ok",
     "timestamp": 1588369057197,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "gH6xGxY9BvKO",
    "outputId": "ee53dd59-c5ea-49ff-fcc6-0c76878736ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19843\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential = True,\n",
    "    batch_first = True,\n",
    "    lower = True,\n",
    "    #tokenize = 'spacy',\n",
    "    pad_first = True\n",
    ")\n",
    "LABEL = ttd.Field(sequential = False, use_vocab = False, is_target = True);\n",
    "dataset = ttd.TabularDataset(path = 'twitter.csv', format = 'csv',\n",
    "                             skip_header = True,\n",
    "                             fields = [('label', LABEL),('data', TEXT)] \n",
    "                             ## it will generate an obj dattset.example.data\n",
    "                             ## and an obj dattset.example.label\n",
    "                             );\n",
    "\n",
    "train_dataset, test_dataset = dataset.split(0.7);## default 0.7 here\n",
    "TEXT.build_vocab(train_dataset);\n",
    "vocab = TEXT.vocab;\n",
    "print(len(vocab));\n",
    "#vocab.stoi\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "print(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FagSvnEFrKL"
   },
   "source": [
    "## split training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6317,
     "status": "ok",
     "timestamp": 1588369059840,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "8xEAlg1JBvKR",
    "outputId": "ae3c0c4a-d732-4baf-9b7d-c3672a797ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9710\n",
      "4161\n",
      "0 torch.Size([32, 23])\n",
      "1 torch.Size([32, 26])\n",
      "2 torch.Size([32, 26])\n",
      "3 torch.Size([32, 25])\n",
      "4 torch.Size([32, 22])\n",
      "10 torch.Size([32, 25])\n",
      "20 torch.Size([32, 25])\n",
      "30 torch.Size([32, 24])\n",
      "40 torch.Size([32, 26])\n",
      "304\n",
      "0 torch.Size([1000, 12])\n",
      "1 torch.Size([1000, 16])\n",
      "2 torch.Size([1000, 19])\n",
      "3 torch.Size([1000, 24])\n",
      "4 torch.Size([161, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = ttd.Iterator.splits(\n",
    "    (train_dataset, test_dataset), \n",
    "    sort_key = lambda x: len(x.data), ## x is this object\n",
    "    batch_sizes = (32,1000),\n",
    "    device = device\n",
    "    );\n",
    "print(len(train_dataset));\n",
    "print(len(test_dataset))\n",
    "printlist = [0,1,2,3,4,10,20,30,40];\n",
    "ind = 0;\n",
    "for inputs, targets in train_iter:\n",
    "    #T_indicator = torch.zeros(len(labels), 2).scatter_(1, labels, 1)\n",
    "    #print(inputs,'\\n',targets);\n",
    "    #break;\n",
    "    if ind in printlist:\n",
    "        print(ind, inputs.shape);\n",
    "        #print(targets)\n",
    "        #print(T_indicator); \n",
    "    ind += 1;\n",
    "print(ind)\n",
    "\n",
    "ind = 0;\n",
    "for inputs, targets in test_iter:\n",
    "    #print(inputs,'\\n',targets);\n",
    "    #break;\n",
    "    if ind in printlist:\n",
    "        print(ind, inputs.shape);\n",
    "    ind += 1;\n",
    "print(ind);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AC1YFTXJWRfy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_29DAXSBvKU"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F;\n",
    "'''\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, n_outputs):\n",
    "        super(CNN,self).__init__();\n",
    "        self.V = n_vocab;\n",
    "        self.D = embed_dim;\n",
    "        self.K = n_outputs;\n",
    "\n",
    "        ## dropout\n",
    "        self.dropout = nn.Dropout(p = 0.5);\n",
    "\n",
    "        ##layers implement\n",
    "        self.embed = nn.Embedding(self.V, self.D);\n",
    "        ## embed layer output is [N T D];\n",
    "\n",
    "        self.conv1 = nn.Conv1d(self.D, 32, 3, padding=1);\n",
    "        ## conv layer 1 output is [N T M];\n",
    "\n",
    "        self.pool1 = nn.MaxPool1d(2);\n",
    "        ## max pooling layer 1 output is [N T2 M]\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, padding = 1);\n",
    "        ## conv layer 2 output is  [N T2 M2]\n",
    "\n",
    "        self.pool2 = nn.MaxPool1d(2);\n",
    "        ## max pooling layer 2 output is [N T3 M2];\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128,3, padding = 1);\n",
    "        ## conv layer 3 output is [N T3 M3]\n",
    "\n",
    "        # flattern / global max pool later \n",
    "        ## output [N M3]\n",
    "        self.fc = nn.Linear(128, self.K);\n",
    "        \n",
    "\n",
    "'''\n",
    "        #in Torch, features come first\n",
    "        #in TF/ NLP, features come last\n",
    "        \n",
    "        #So Before and After Convolution layer, we HAVE to reshape! \n",
    "'''\n",
    "    def forward(self,X):\n",
    "        ## input is [N T]\n",
    "        # embed layer\n",
    "        out = self.embed(X); ## output is [N T D]\n",
    "\n",
    "        out = out.permute(0,2,1); ## out is [N D T], feature first\n",
    "        out = self.conv1(out); ## out is [N M T]\n",
    "        out = F.relu(out); ## [N M T]\n",
    "        out = self.dropout(out);\n",
    "        out = self.pool1(out); ## out is [N M T2];\n",
    "\n",
    "        out = self.conv2(out); ## out is [N M2 T2]\n",
    "        out = F.relu(out); ## [N M2 T2]\n",
    "        out = self.dropout(out);\n",
    "        out = self.pool2(out); ## out is [N M2 T3];\n",
    "\n",
    "        out = self.conv3(out); ## [N M3 T3];\n",
    "        out = self.dropout(out);\n",
    "\n",
    "        out = out.permute(0,2,1); ## [N T3 M3];\n",
    "        out,_ = torch.max(out,1); ## [N M3]; Global Max pool\n",
    "        \n",
    "        out = F.relu(out);\n",
    "\n",
    "        ## final dense layer\n",
    "        out = self.fc(out);\n",
    "        #out = F.log_softmax(out,dim=1)\n",
    "        return out;\n",
    "''';\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim,  n_outputs):\n",
    "        super(CNN,self).__init__();\n",
    "\n",
    "        self.V = n_vocab;\n",
    "        self.D = embed_dim;\n",
    "        \n",
    "        self.K = n_outputs;\n",
    "        ## dropout\n",
    "        #self.dropout = nn.Dropout(p = 0.25);\n",
    "\n",
    "        ## before this mode is sent to GPU, we have to define its dimensions first\n",
    "        ## so you cannot using call any parameters like self.V self.D in this forward \n",
    "        ##function\n",
    "        self.embed = nn.Embedding(self.V, self.D); \n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv1d(self.D, 32, 3, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, 3, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(64, 128,3, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            #nn.Linear(128, self.K1);\n",
    "        );\n",
    "\n",
    "        self.fully_connect = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),            \n",
    "\n",
    "            nn.Linear(32, self.K),\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "\n",
    "\n",
    "    def forward(self,X):\n",
    "        #out = self.embed(X); ## output is [N T D]\n",
    "        out = self.embed(X);\n",
    "\n",
    "        out = out.permute(0,2,1); ## out is [N D T], feature first\n",
    "        out = self.conv_layer(out);\n",
    "        out = out.permute(0,2,1); ## [N T3 M3];\n",
    "\n",
    "        out,_ = torch.max(out,1); ## [N M3]; Global Max pool\n",
    "        out = self.fully_connect(out);\n",
    "        return out;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing a classification with 3 label(s)!\n",
      "\n",
      "Yizhou said 欲速则不达，施主稍安勿躁: | ▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ | 3.3%\n",
      "Epoch (1 / 30)...Train_Loss: 6.980e-01...Test_loss: 6.958e-01...Duration: 1.043e+01 sec\n",
      "Yizhou said 欲速则不达，施主稍安勿躁: | ▒▒▒▒▒▒▒░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ | 13.3%\n",
      "Epoch (4 / 30)...Train_Loss: 6.957e-01...Test_loss: 6.956e-01...Duration: 5.302e+01 sec"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8426b178e847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mSTD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYZ_torch_std\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mSTD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOptimizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Yizhou_coding\\Coursework\\2020 SP course\\CS 760\\Project\\GitHub\\0_group work\\CNN\\YZ_nn_training_format.py\u001b[0m in \u001b[0;36mOptimizing\u001b[1;34m(self, lr, criterion, optimizer, epochs, plot_epoch)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStep_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStep_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'testing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Yizhou_coding\\Coursework\\2020 SP course\\CS 760\\Project\\GitHub\\0_group work\\CNN\\YZ_nn_training_format.py\u001b[0m in \u001b[0;36mStep_gradient_descent\u001b[1;34m(self, data_iter, process)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprocess\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'training'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import YZ_nn_training_format\n",
    "from YZ_nn_training_format import YZ_torch_std;\n",
    "model = CNN(len(vocab), 50, K_class);\n",
    "STD = YZ_torch_std(model, device, train_iter, test_iter, K_class = K_class);\n",
    "STD.Optimizing(epochs = 30, plot_epoch = 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HakdL5EhBvKX"
   },
   "outputs": [],
   "source": [
    "model = CNN(len(vocab), 50, K_class);\n",
    "model.to(device);\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(); ## Python can even return a function\n",
    "#criterion = nn.CrossEntropyLoss();\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=1e-3);\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-3, momentum = 0.5);\n",
    "\n",
    "def batch_gd(model, criterion, optimizer, train_iter, test_iter, epochs, print_epoch = 20):\n",
    "    start = time.time();\n",
    "    \n",
    "    train_losses = np.zeros(epochs);\n",
    "    test_losses = np.zeros(epochs);\n",
    "\n",
    "    for it in range(epochs):\n",
    "        #if it%print_epoch  == 0:\n",
    "            #now = time.process_time();\n",
    "        \n",
    "        train_loss = [];\n",
    "        \n",
    "        for inputs, targets in train_iter:\n",
    "            #targets = targets.view(-1,1).float();\n",
    "            #targets = targets.view(-1,1).int();\n",
    "\n",
    "            targets = torch.nn.functional.one_hot(targets, K_class).float();   \n",
    "\n",
    "            ## move data to GPU\n",
    "            inputs, targets  =  inputs.to(device), targets.to(device);\n",
    "\n",
    "            ## zero the parameter gradient\n",
    "            optimizer.zero_grad();\n",
    "\n",
    "            ## forward pass\n",
    "            outputs = model(inputs);\n",
    "\n",
    "            loss = criterion(outputs, targets);\n",
    "\n",
    "            ## back probagation\n",
    "            loss.backward();\n",
    "            optimizer.step();\n",
    "            train_loss.append(loss.item());\n",
    "\n",
    "        train_loss = np.mean(train_loss);\n",
    "\n",
    "        test_loss = [];\n",
    "        for inputs, targets in test_iter:\n",
    "            #targets = targets.view(-1,1).float();\n",
    "            targets = torch.nn.functional.one_hot(targets, K_class).float();\n",
    "            ## move data to GPU\n",
    "            inputs, targets  =  inputs.to(device), targets.to(device);\n",
    "\n",
    "            ## zero the parameter gradient\n",
    "            optimizer.zero_grad();\n",
    "\n",
    "            ## forward pass,  no need to back\n",
    "            outputs = model(inputs);\n",
    "            loss = criterion(outputs, targets);\n",
    "            test_loss.append(loss.item());\n",
    "        \n",
    "        test_loss = np.mean(test_loss);\n",
    "\n",
    "        train_losses[it] = train_loss;\n",
    "        test_losses[it] = test_loss;\n",
    "\n",
    "        if it%print_epoch  == 0:\n",
    "            dt = time.time() - start;\n",
    "            nn_comments = \"Epoch (%d / %d)...Train_Loss: %.3e...Test_loss: %.3e...Duration: %.3e sec\"\\\n",
    "                %(it+1, epochs, train_loss, test_loss, dt);\n",
    "\n",
    "            Process_bar((it+1)/epochs*1.0, comments=nn_comments, overwrite = False);\n",
    "    Process_bar(1.0, comments='Done!', overwrite = False);\n",
    "    \n",
    "    return train_losses, test_losses;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42031,
     "status": "ok",
     "timestamp": 1588369096192,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "pCJJ-csbBvKa",
    "outputId": "f6e6ccf0-5836-48c9-9328-5af4163597b0"
   },
   "outputs": [],
   "source": [
    "CNN_epochs = 30;\n",
    "train_losses, test_losses = batch_gd(model, criterion, optimizer, train_iter, test_iter, CNN_epochs,3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41874,
     "status": "ok",
     "timestamp": 1588369096193,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "f3pwahebBvKe",
    "outputId": "533bb17f-6342-467b-f41c-5ed27c71d0b8"
   },
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "plt.step(range(len(train_losses)),train_losses, c = 'r', label = 'Train_Error');\n",
    "plt.step(range(len(test_losses)),test_losses, c = 'b', label  = 'Test_Error');\n",
    "plt.legend();\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Loss');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42408,
     "status": "ok",
     "timestamp": 1588369096884,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "ly58CjdrBvKg",
    "outputId": "04f710e3-337e-4160-d305-61e3f6fcd30a"
   },
   "outputs": [],
   "source": [
    "## Accuracy:\n",
    "n_correct = 0.0;\n",
    "n_total = 0.0;\n",
    "\n",
    "for inputs, targets in train_iter:\n",
    "    #targets = targets.view(-1,1).float();\n",
    "    targets = torch.nn.functional.one_hot(targets, K_class).float(); \n",
    "    outputs = model(inputs);\n",
    "    prediction = (torch.argmax(outputs, dim=1));\n",
    "\n",
    "    n_total += targets.shape[0];\n",
    "    n_correct += (torch.argmax(targets, dim=1) == prediction).sum().item();\n",
    "\n",
    "train_acc = n_correct/n_total;\n",
    "\n",
    "n_correct = 0.0;\n",
    "n_total = 0.0;\n",
    "for inputs, targets in test_iter:\n",
    "    #targets = targets.view(-1,1).float();\n",
    "    targets = torch.nn.functional.one_hot(targets, K_class).float(); \n",
    "    outputs = model(inputs);\n",
    "    #prediction = (outputs > 0);\n",
    "    prediction = (torch.argmax(outputs, dim=1));\n",
    "\n",
    "    n_total += targets.shape[0];\n",
    "    n_correct += (torch.argmax(targets, dim=1) == prediction).sum().item();\n",
    "\n",
    "test_acc = n_correct/n_total;\n",
    "\n",
    "print(\"Training Accuracy = %.3e \\nTesting Accuracy = %.3e\" %(train_acc,\n",
    "                                                             test_acc));\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rft9fGqjBvKp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zktC9_u6ZElq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vRA7rN-Zax4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN sentiment test1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

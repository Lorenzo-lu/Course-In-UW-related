{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUJp0HI9ZIMg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDTqepk6Z0LN"
   },
   "outputs": [],
   "source": [
    "import sys;\n",
    "import torch;\n",
    "import torch.nn as nn;\n",
    "import torchtext.data as ttd;\n",
    "from torchtext.vocab import GloVe;\n",
    "\n",
    "import numpy as np;\n",
    "import matplotlib.pyplot as plt;\n",
    "import pandas as pd;\n",
    "pd.options.mode.chained_assignment = None; ## avoid warning\n",
    "#from datetime import datetime;\n",
    "import time;\n",
    "\n",
    "class YZ_torch_std:\n",
    "    def __init__(self, model,  device, train_iter, test_iter, \n",
    "                 job = 'classification', K_class = 2):\n",
    "        self.model = model;\n",
    "        self.device = device;\n",
    "        self.train_iter = train_iter;\n",
    "        self.test_iter = test_iter;\n",
    "\n",
    "        self.job = job;\n",
    "        self.K_class = K_class;\n",
    "\n",
    "        if self.job == 'regression':\n",
    "            self.K_class = 1;\n",
    "\n",
    "        print(\"Doing a \" + job + \"with\" + str(self.K_class) + \"label(s)\");\n",
    "\n",
    "        self.model.to(self.device);\n",
    "\n",
    "        self.train_losses = [];\n",
    "        self.test_losses = [];\n",
    "        \n",
    "\n",
    "    ## make a process bar\n",
    "    def YZ_process_bar(self, ratio, comments = False, overwrite = True, length = 50):\n",
    "        bar = 'Yizhou said 欲速则不达，施主稍安勿躁: | ';\n",
    "        i = 0;\n",
    "        while i < ratio * length:\n",
    "            bar += '▒';\n",
    "            i += 1;\n",
    "        while i < length:\n",
    "            bar += '░';\n",
    "            i += 1;        \n",
    "        bar += (' | %s%%'%(int(ratio*1000)/10));\n",
    "        if ratio == 1:\n",
    "            bar += ' (^_^)/ Done!'\n",
    "        if comments != False:\n",
    "            bar += ('\\n' + str(comments));\n",
    "        if overwrite == True:\n",
    "            print('\\r', end='');\n",
    "        else:\n",
    "            print('\\n',end = '');\n",
    "        print(bar, end='');\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    ## GD\n",
    "    def Step_gradient_descent(self, data_iter, process = 'testing'):\n",
    "        loss_list = [];\n",
    "        for inputs, targets in data_iter:\n",
    "            if self.job == 'classification':\n",
    "                targets = torch.nn.functional.one_hot(targets, self.K_class).float();   \n",
    "            elif self.job == 'regression':\n",
    "                targets = targets.view(-1,1).float();\n",
    "            inputs, targets  =  inputs.to(self.device), targets.to(self.device);\n",
    "            self.optimizer.zero_grad();\n",
    "            outputs = self.model(inputs);\n",
    "            loss = self.criterion(outputs, targets);\n",
    "\n",
    "            if process == 'training':\n",
    "                loss.backward();\n",
    "                self.optimizer.step();\n",
    "\n",
    "            loss_list.append(loss.item());\n",
    "        return np.mean(loss_list);\n",
    "\n",
    "    def Classification_rate(self, data_iter):\n",
    "        n_correct = 0.0;\n",
    "        n_total = 0.0;\n",
    "        for inputs, targets in data_iter:\n",
    "            targets = torch.nn.functional.one_hot(targets, self.K_class).float();\n",
    "            outputs = self.model(inputs);\n",
    "            prediction = (torch.argmax(outputs, dim=1));\n",
    "\n",
    "            n_total += targets.shape[0];\n",
    "            n_correct += (torch.argmax(targets, dim=1) == prediction).sum().item();\n",
    "            rate = n_correct / n_total;\n",
    "        print(\"The classification rate for this dataset is %s%%\" %(int(rate * 1000)/10));\n",
    "        return rate;\n",
    "\n",
    "\n",
    "    def Optimizing(self, lr = 1e-3, criterion = False, optimizer = False, \n",
    "                   epochs = False, plot_epoch = False):\n",
    "        if criterion == False:\n",
    "            criterion = nn.BCEWithLogitsLoss();\n",
    "        self.criterion = criterion;\n",
    "        if optimizer == False:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr);\n",
    "        self.optimizer = optimizer;\n",
    "        if epochs == False:\n",
    "            epochs = 10;\n",
    "        if plot_epoch == False:\n",
    "            plot_epoch = int(epochs/10);\n",
    "        if plot_epoch <= 1:\n",
    "            plot_epoch = 1;\n",
    "\n",
    "        self.performance = []; ## record time\n",
    "        start = time.time();## set the timer starting!\n",
    "\n",
    "        for it in range(epochs):\n",
    "            train_loss = self.Step_gradient_descent(train_iter, process='training');\n",
    "            test_loss = self.Step_gradient_descent(train_iter, process='testing');\n",
    "\n",
    "            if it%plot_epoch  == 0:\n",
    "                dt = time.time() - start;\n",
    "                nn_comments = \"Epoch (%d / %d)...Train_Loss: %.3e...Test_loss: %.3e...Duration: %.3e sec\"\\\n",
    "                %(it+1, epochs, train_loss, test_loss, dt);\n",
    "                self.YZ_process_bar((it+1)/epochs*1.0, comments=nn_comments, overwrite = False);\n",
    "\n",
    "                self.train_losses.append(train_loss);\n",
    "                self.test_losses.append(test_loss);\n",
    "                self.performance.append(dt);\n",
    "\n",
    "            if it == epoch-1:                \n",
    "                self.YZ_process_bar((it+1)/epochs*1.0, overwrite = False);\n",
    "        \n",
    "        plt.figure();\n",
    "        plt.step(range(len(self.train_losses)), self.train_losses, c = 'r', \n",
    "                 label = 'training');\n",
    "        plt.step(range(len(self.test_losses)), self.test_losses, c = 'b',\n",
    "                 label = 'testing');\n",
    "        plt.xlabel('Epochs/%s'%(plot_epoch));\n",
    "        plt.ylabel('Loss');\n",
    "        plt.show();\n",
    "\n",
    "        if self.job == 'classification':\n",
    "            print(\"Training:\");\n",
    "            self.CR_train = self.Classification_rate(training_iter);\n",
    "            print(\"Testing:\");\n",
    "            self.CR_test = self.Classification_rate(test_iter);\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1588379195756,
     "user": {
      "displayName": "YIZHOU LU",
      "photoUrl": "",
      "userId": "12883021123371670841"
     },
     "user_tz": 300
    },
    "id": "ufoq_7usqMoQ",
    "outputId": "8d8f2af9-0406-45f8-c760-4da78ade9e61"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzUzy3a5xxOK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2rH0/Tchq9z4S2pSLSJBg",
   "collapsed_sections": [],
   "name": "YZ_nn_training_format",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
